{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c71be41",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ•µï¸ Day 13 â€” Credit Card Fraud Detection (Advanced Hybrid)\n",
    "\n",
    "**Goal:** Detect fraudulent transactions with a **hybrid pipeline**:\n",
    "\n",
    "1) **Unsupervised anomaly scores**  \n",
    "\n",
    "   â€¢ Isolation Forest (outlierness)  \n",
    "\n",
    "   â€¢ Autoencoder (reconstruction error)  \n",
    "\n",
    "2) **Supervised classifier** (XGBoost) on an enriched feature set  \n",
    "\n",
    "3) **Imbalance handling** with SMOTE  \n",
    "\n",
    "4) **Threshold optimization** for **cost-sensitive** business goals  \n",
    "\n",
    "5) **Evaluation** via Confusion Matrix, ROC, **Precisionâ€“Recall**, and **SHAP** explanations\n",
    "\n",
    "**Dataset:** Kaggle â€œCredit Card Fraud Detectionâ€ â€” 284,807 transactions, **0.17% fraud**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c81b5",
   "metadata": {},
   "source": [
    "## ðŸ”§ 1) Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc5235",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If needed on Kaggle, uncomment:\n",
    "# !pip install -q optuna imbalanced-learn shap xgboost tensorflow\n",
    "\n",
    "import os, gc, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, roc_auc_score,\n",
    "                             precision_recall_curve, roc_curve, auc)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "RNG = 42\n",
    "np.random.seed(RNG)\n",
    "tf.random.set_seed(RNG)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (6,4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1066c1d7",
   "metadata": {},
   "source": [
    "## ðŸ“¥ 2) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CANDIDATES = [\n",
    "    \"/kaggle/input/creditcardfraud/creditcard.csv\",\n",
    "    \"/kaggle/input/credit-card-fraud-detection/creditcard.csv\",\n",
    "    \"creditcard.csv\"\n",
    "]\n",
    "data_path = next((p for p in CANDIDATES if os.path.exists(p)), None)\n",
    "if data_path is None:\n",
    "    raise SystemExit(\"âŒ Dataset not found. Add the Kaggle dataset â€˜mlg-ulb/creditcardfraudâ€™ to the notebook.\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84278fe",
   "metadata": {},
   "source": [
    "## ðŸ”Ž 3) Basic EDA & Target Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df547264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df[\"Class\"].value_counts().rename({0:\"Not Fraud\", 1:\"Fraud\"}))\n",
    "print(\"Fraud ratio:\", df[\"Class\"].mean())\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "sns.countplot(x=\"Class\", data=df, ax=ax[0])\n",
    "ax[0].set_title(\"Target Distribution\")\n",
    "\n",
    "sns.histplot(df[\"Amount\"], bins=50, ax=ax[1])\n",
    "ax[1].set_title(\"Amount Distribution\")\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d27d6",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ 4) Train/Test Split & Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9bed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Features: V1..V28 + Amount (drop Time)\n",
    "X = df.drop(columns=[\"Class\", \"Time\"])\n",
    "y = df[\"Class\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RNG\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a295cf",
   "metadata": {},
   "source": [
    "## ðŸŒ² 5) Unsupervised Signal 1 â€” Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5f8e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iso = IsolationForest(\n",
    "    n_estimators=300, contamination=\"auto\",\n",
    "    random_state=RNG, n_jobs=-1\n",
    ")\n",
    "iso.fit(X_train_scaled)\n",
    "\n",
    "# Decision function: higher = more normal. Convert to anomaly score.\n",
    "train_iso_raw = -iso.decision_function(X_train_scaled)\n",
    "test_iso_raw  = -iso.decision_function(X_test_scaled)\n",
    "\n",
    "def minmax01(a):\n",
    "    lo, hi = np.min(a), np.max(a)\n",
    "    return (a - lo) / (hi - lo + 1e-9)\n",
    "\n",
    "train_iso = minmax01(train_iso_raw)\n",
    "test_iso  = minmax01(test_iso_raw)\n",
    "\n",
    "pd.Series(train_iso).describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e805ec",
   "metadata": {},
   "source": [
    "## ðŸ¤– 6) Unsupervised Signal 2 â€” Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da405e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the autoencoder only on NON-FRAUD class (learn \"normal\" patterns)\n",
    "X_train_norm = X_train_scaled[y_train == 0]\n",
    "\n",
    "input_dim = X_train_norm.shape[1]\n",
    "encoding_dim = 16  # bottleneck\n",
    "\n",
    "ae = keras.Sequential([\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(encoding_dim, activation=\"relu\", name=\"bottleneck\"),\n",
    "    layers.Dense(32, activation=\"relu\"),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(input_dim, activation=None)\n",
    "])\n",
    "\n",
    "ae.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "history = ae.fit(\n",
    "    X_train_norm, X_train_norm,\n",
    "    epochs=20, batch_size=512,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
    "plt.title(\"Autoencoder Training Loss\"); plt.legend(); plt.show()\n",
    "\n",
    "def recon_err(model, X):\n",
    "    X_hat = model.predict(X, verbose=0)\n",
    "    return np.mean((X - X_hat)**2, axis=1)\n",
    "\n",
    "train_recon = recon_err(ae, X_train_scaled)\n",
    "test_recon  = recon_err(ae, X_test_scaled)\n",
    "\n",
    "train_recon = minmax01(train_recon)\n",
    "test_recon  = minmax01(test_recon)\n",
    "\n",
    "pd.Series(train_recon).describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdddb3cd",
   "metadata": {},
   "source": [
    "## ðŸ§± 7) Build Enriched Feature Set for Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c353b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_enriched = np.hstack([X_train_scaled, train_iso.reshape(-1,1), train_recon.reshape(-1,1)])\n",
    "X_test_enriched  = np.hstack([X_test_scaled,  test_iso.reshape(-1,1),  test_recon.reshape(-1,1)])\n",
    "\n",
    "enriched_cols = list(X_train.columns) + [\"IForest_Anomaly\", \"AE_ReconErr\"]\n",
    "print(\"Enriched feature count:\", X_train_enriched.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0e4de",
   "metadata": {},
   "source": [
    "## âš–ï¸ 8) Handle Imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977fabf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sm = SMOTE(random_state=RNG, k_neighbors=5, n_jobs=-1)\n",
    "X_train_bal, y_train_bal = sm.fit_resample(X_train_enriched, y_train)\n",
    "print(\"Before:\", np.bincount(y_train))\n",
    "print(\"After: \", np.bincount(y_train_bal))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc47d23",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 9) XGBoost with Optuna (optimize ROC-AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 400, 1400),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.2, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-2, 10.0, log=True),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"random_state\": RNG,\n",
    "        \"n_jobs\": -1,\n",
    "        \"tree_method\": \"hist\"\n",
    "    }\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=RNG)\n",
    "    aucs = []\n",
    "    for tr_idx, va_idx in skf.split(X_train_bal, y_train_bal):\n",
    "        X_tr, X_va = X_train_bal[tr_idx], X_train_bal[va_idx]\n",
    "        y_tr, y_va = y_train_bal[tr_idx], y_train_bal[va_idx]\n",
    "        clf = xgb.XGBClassifier(**params)\n",
    "        clf.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False, early_stopping_rounds=50)\n",
    "        va_proba = clf.predict_proba(X_va)[:,1]\n",
    "        aucs.append(roc_auc_score(y_va, va_proba))\n",
    "    return float(np.mean(aucs))\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=False)\n",
    "\n",
    "print(\"Best AUC:\", study.best_value)\n",
    "print(\"Best params:\", json.dumps(study.best_params, indent=2))\n",
    "best_params = study.best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9c2df",
   "metadata": {},
   "source": [
    "## âœ… 10) Train Best Model & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd1744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_params.update(dict(eval_metric=\"auc\", random_state=RNG, n_jobs=-1, tree_method=\"hist\"))\n",
    "clf = xgb.XGBClassifier(**best_params)\n",
    "clf.fit(X_train_bal, y_train_bal, eval_set=[(X_test_enriched, y_test)], verbose=False)\n",
    "\n",
    "proba = clf.predict_proba(X_test_enriched)[:,1]\n",
    "roc = roc_auc_score(y_test, proba)\n",
    "print(f\"Test ROC-AUC: {roc:.4f}\")\n",
    "\n",
    "# Default threshold 0.5\n",
    "pred_05 = (proba >= 0.5).astype(int)\n",
    "cm = confusion_matrix(y_test, pred_05)\n",
    "print(\"Confusion Matrix @0.5:\\n\", cm)\n",
    "print(classification_report(y_test, pred_05, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76e926",
   "metadata": {},
   "source": [
    "## ðŸ’° 11) Threshold Optimization (Cost-Sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Business costs: FN (missed fraud) >> FP (manual review)\n",
    "C_FP = 1    # cost of investigating a legit transaction\n",
    "C_FN = 10   # cost of missing an actual fraud\n",
    "\n",
    "def find_best_threshold(y_true, y_proba, c_fp=C_FP, c_fn=C_FN):\n",
    "    thresholds = np.linspace(0.01, 0.99, 99)\n",
    "    best = {\"thr\":0.5, \"cost\":np.inf, \"pr\":0, \"rc\":0}\n",
    "    for t in thresholds:\n",
    "        pred = (y_proba >= t).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, pred).ravel()\n",
    "        cost = fp*c_fp + fn*c_fn\n",
    "        pr = tp / (tp + fp + 1e-9)\n",
    "        rc = tp / (tp + fn + 1e-9)\n",
    "        if cost < best[\"cost\"]:\n",
    "            best = {\"thr\":t, \"cost\":cost, \"pr\":pr, \"rc\":rc}\n",
    "    return best\n",
    "\n",
    "best = find_best_threshold(y_test, proba)\n",
    "best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f66b16",
   "metadata": {},
   "source": [
    "## ðŸ“‰ 12) Final Evaluation @ Optimized Threshold + Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7471205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thr = best[\"thr\"]\n",
    "pred_opt = (proba >= thr).astype(int)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred_opt).ravel()\n",
    "\n",
    "print(f\"Chosen threshold: {thr:.2f}\")\n",
    "print(\"Confusion Matrix @thr={:.2f}:\\n\".format(thr), np.array([[tn, fp],[fn, tp]]))\n",
    "print(classification_report(y_test, pred_opt, digits=4))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, proba):.4f}\")\n",
    "\n",
    "# Curves\n",
    "prec, rec, pr_thr = precision_recall_curve(y_test, proba)\n",
    "plt.plot(rec, prec); plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Precisionâ€“Recall Curve\"); plt.show()\n",
    "\n",
    "fpr, tpr, roc_thr = roc_curve(y_test, proba)\n",
    "plt.plot(fpr, tpr, label=f\"AUC={auc(fpr,tpr):.3f}\")\n",
    "plt.plot([0,1],[0,1],'--',c='gray'); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.legend(); plt.title(\"ROC Curve\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bdc1d",
   "metadata": {},
   "source": [
    "## ðŸ§  13) SHAP Explainability (Top Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c04b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SHAP on a sample for speed\n",
    "explainer = shap.TreeExplainer(clf)\n",
    "sample_idx = np.random.choice(len(X_test_enriched), size=min(3000, len(X_test_enriched)), replace=False)\n",
    "shap_values = explainer.shap_values(X_test_enriched[sample_idx])\n",
    "\n",
    "feature_names = enriched_cols\n",
    "shap.summary_plot(shap_values, X_test_enriched[sample_idx], feature_names=feature_names, show=False)\n",
    "plt.title(\"SHAP Summary â€” Top Drivers of Fraud\"); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fcb083",
   "metadata": {},
   "source": [
    "\n",
    "## ðŸ§  Key Takeaways\n",
    "- **Hybrid works**: Unsupervised signals (Isolation Forest + Autoencoder) improved the supervised classifier.\n",
    "- **Imbalance matters**: SMOTE helped the classifier learn minority patterns.\n",
    "- **Cost-sensitive threshold** beats accuracy: we optimized the cut-off to **reduce missed fraud** (FN) at acceptable FP review cost.\n",
    "- **Explainability** (SHAP) shows which signals push a transaction towards *fraud* (e.g., anomaly scores, Amount, specific PCA components).\n",
    "\n",
    "## ðŸ’¼ Business Interpretation\n",
    "- **False Negatives** are expensive â†’ tune threshold to **maximize recall** within operational FP limits.\n",
    "- Use model outputs to **prioritize manual review** queues.\n",
    "- Continuously retrain with **recent fraud patterns** (concept drift).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec83a83",
   "metadata": {},
   "source": [
    "## ðŸ’¾ 15) Save Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10efbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib, json\n",
    "\n",
    "joblib.dump(scaler, \"scaler.joblib\")\n",
    "joblib.dump(iso, \"isolation_forest.joblib\")\n",
    "ae.save(\"autoencoder.h5\")\n",
    "joblib.dump(clf, \"xgb_hybrid.joblib\")\n",
    "\n",
    "with open(\"feature_names.json\",\"w\") as f:\n",
    "    json.dump(feature_names, f)\n",
    "\n",
    "print(\"Saved: scaler.joblib, isolation_forest.joblib, autoencoder.h5, xgb_hybrid.joblib, feature_names.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923577b4",
   "metadata": {},
   "source": [
    "## ðŸ”® 16) Inference Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44579516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fraud_inference(raw_row_dict, thr_override=None):\n",
    "    \"\"\"raw_row_dict: { 'V1':..., 'V2':..., ..., 'V28':..., 'Amount':... }\"\"\"\n",
    "    cols = list(X.columns)  # original order\n",
    "    x_raw = pd.DataFrame([raw_row_dict], columns=cols)\n",
    "    xs = scaler.transform(x_raw)\n",
    "    iso_score = minmax01(-iso.decision_function(xs)).reshape(-1,1)\n",
    "    # reuse AE reconstruction calculation\n",
    "    ae_err = minmax01(np.mean((xs - ae.predict(xs, verbose=0))**2, axis=1)).reshape(-1,1)\n",
    "    xe = np.hstack([xs, iso_score, ae_err])\n",
    "\n",
    "    p = clf.predict_proba(xe)[:,1][0]\n",
    "    thr = best[\"thr\"] if thr_override is None else thr_override\n",
    "    lab = int(p >= thr)\n",
    "    return float(p), lab\n",
    "\n",
    "# Demo on a real test row\n",
    "demo = X_test.iloc[0].to_dict()\n",
    "fraud_inference(demo)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
